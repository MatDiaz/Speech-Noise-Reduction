{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca22ec7",
   "metadata": {},
   "source": [
    "Descripción general del sistema de Deep Learning\n",
    "\n",
    "El sistema está diseñado para realizar reducción de ruido en audio mediante técnicas de deep learning. La entrada del modelo corresponde al espectro de magnitudes de la señal de audio con ruido, mientras que la salida objetivo corresponde al espectro de magnitudes de la señal limpia. La red neuronal toma como entrada el espectro de magnitudes con ruido y estima el espectro de magnitudes de la señal denoised.\n",
    "El entrenamiento se realiza minimizando el error cuadrático medio (MSE) entre el espectro predicho y el espectro objetivo. La arquitectura de la red está compuesta por capas totalmente conectadas que procesan segmentos de la señal en el dominio frecuencia–tiempo.\n",
    "\n",
    "Tras la inferencia, la señal denoised se reconstruye en el dominio del tiempo combinando el espectro de magnitudes predicho con la información de fase original de la señal con ruido. Esta reconstrucción se lleva a cabo mediante la transformada rápida de Fourier inversa de ventana corta (ISTFT). El resultado es una forma de onda con el ruido significativamente reducido, y que se aproxima a la señal limpia de referencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0226f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import Utils.model_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf891e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([160586, 1, 129, 8])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 256\n",
    "model_params= {\"window_length\": window_size,\n",
    "\"overlap\": int(0.75 * window_size),\n",
    "\"fft_length\": window_size,\n",
    "\"num_features\": window_size // 2 + 1,\n",
    "\"num_segments\": 8,\n",
    "\"batch_size\": 128,\n",
    "\"epochs\": 3,\n",
    "\"lr\": 1e-5}\n",
    "\n",
    "clean_folder = \"../Audios/Dataset/dev-clean\"\n",
    "\n",
    "noisy_folder = \"../Audios/Augmented Audios 3 SNR\"\n",
    "\n",
    "noisy_segments_list, clean_segments_list = Utils.model_setup.initialize(model_params, clean_folder, noisy_folder)\n",
    "\n",
    "X, y = Utils.model_setup.batch_preprocessing(clean_segments_list, noisy_segments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5fb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_samples = X.shape[0]\n",
    "indices = torch.randperm(num_samples)\n",
    "train_len = int(0.99 * num_samples)\n",
    "train_idx = indices[:train_len]\n",
    "val_idx = indices[train_len:]\n",
    "lr=1e-5\n",
    "batch_size=128\n",
    "num_segments=8\n",
    "num_features=129\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X[train_idx], y[train_idx])\n",
    "val_dataset = torch.utils.data.TensorDataset(X[val_idx], y[val_idx])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "model = Utils.model_setup.DenoiseNet(num_features, num_segments).to(device)\n",
    "\n",
    "# ======================\n",
    "# 7. Entrenamiento\n",
    "# ======================\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0cf4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 0.316396 - Val Loss: 0.132619\n",
      "Epoch 2/3 - Train Loss: 0.136404 - Val Loss: 0.084592\n",
      "Epoch 3/3 - Train Loss: 0.106992 - Val Loss: 0.069854\n"
     ]
    }
   ],
   "source": [
    "epochs=3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a43568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model, \"denoise_model_complete.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a5699",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clean_waveform\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Ejemplo con una muestra del dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m waveform, sr, *_ = \u001b[43mdataset\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     44\u001b[39m noisy_input = add_noise(waveform, snr_db=\u001b[32m5\u001b[39m)\n\u001b[32m     45\u001b[39m denoised_output = denoise_audio(model, noisy_input)\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_noise_reduction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
